[SERVICE]
# https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/configuration-file#config_section
    Flush         1
    Log_Level     info
    Daemon        off
    Parsers_File  parsers.conf
    HTTP_Server   On
    HTTP_Listen   0.0.0.0
    HTTP_Port     2020

[INPUT]
# https://docs.fluentbit.io/manual/pipeline/inputs/tail
    Name              tail
    Tag               app.*
    Path              ${APP_LOG_DIR}/${APP_LOG_FILE_WATCH_PATTERN}
    Parser            json
    DB                /var/local/fluent-bit/flb_kube_1.db
    Mem_Buf_Limit     ${FLB_INPUT_MEM_BUF_LIMIT}
    Buffer_Chunk_Size ${FLB_INPUT_BUFFER_CHUNK_SIZE}
    Buffer_Max_Size   ${FLB_INPUT_BUFFER_MAX_SIZE}
    Skip_Long_Lines   On
    Refresh_Interval  ${FLB_INPUT_REFRESH_INTERVAL}
    Key               unstructured_log
    Docker_Mode       Off

[INPUT]
# https://docs.fluentbit.io/manual/pipeline/inputs/tcp
    Name              tcp
    Tag               app.*
    Listen            ${TCP_LOG_HOST}
    Port              ${TCP_LOG_PORT}
    Chunk_Size        ${FLB_INPUT_BUFFER_CHUNK_SIZE}
    Buffer_Size       ${FLB_INPUT_BUFFER_MAX_SIZE}
    Format            json
    Mem_Buf_Limit     ${FLB_INPUT_MEM_BUF_LIMIT}

[FILTER]
# https://docs.fluentbit.io/manual/pipeline/filters/record-modifier
    # Удаление неструктурированных под формат json логов
    Name record_modifier
    Match *
    Remove_key  unstructured_log

[FILTER]
    Name record_modifier
    Match *
    Record envType K8S
    Record namespace ${POD_NAMESPACE}
    Record podName ${POD_NAME}
    Record TEC.nodeName ${NODE_NAME}
    Record TEC.podIp ${POD_IP}
    Record target_es_index ${TARGET_ES_INDEX}

[FILTER]
# https://docs.fluentbit.io/manual/pipeline/filters/nest
    Name nest
    Match *
    Operation nest
    Wildcard TEC.*
    Nest_under tec
    Remove_prefix TEC.

[OUTPUT]
# https://docs.fluentbit.io/manual/pipeline/outputs/kafka
# https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
    Name        kafka
    Match       *
    Brokers     ${KAFKA_BROKER}
    Topics      ${KAFKA_TOPIC}
    Format      json
    # Позволяет не терять логи раньше, чем закончится буфер.
    # И даже в этом случае мы будем терять более поздние записи (потому что они не будут попадать в буфер), а не
    # рандомные (потому что когда Retry_Limit истекает - небольшая часть буфера освобождается и туда попадает произвольный
    # кусок свежих логов).
    Retry_Limit false
    rdkafka.message.timeout.ms ${RDKAFKA_MESSAGE_TIMEOUT_MS}
    rdkafka.queue.buffering.max.kbytes ${RDKAFKA_QUEUE_BUFFERING_MAX_KBYTES}
    #rdkafka.security.protocol ssl
    #rdkafka.ssl.ca.location ${RDKAFKA_SSL_CA_LOCATION}
    #rdkafka.ssl.certificate.location ${RDKAFKA_SSL_CERTIFICATE_LOCATION}
    #rdkafka.ssl.key.location ${RDKAFKA_SSL_KEY_LOCATION}
    #rdkafka.enable.ssl.certificate.verification true